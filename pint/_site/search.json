[
  {
    "objectID": "pint_prd2.html",
    "href": "pint_prd2.html",
    "title": "PINT - Prediction Practical 2",
    "section": "",
    "text": "This R practical will discuss some shrinkage methods for prediction models in R. It assumes that you are aware of how prediction models are developed in R, as discussed in the first practical.",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#packages",
    "href": "pint_prd2.html#packages",
    "title": "PINT - Prediction Practical 2",
    "section": "3.1 Packages",
    "text": "3.1 Packages\nAs usual, before we get going, we will load some useful packages.\n\n# Loading packages\npacman::p_load(\"dplyr\",         # Data wrangling\n               \"magrittr\",      # More efficient pipelines\n               \"tidyr\",         # Data tidying\n               \"rio\",           # Importing data\n               \"glmnet\",        # Shrinkage methods\n               \"pROC\",          # C-statistics\n               \"knitr\",         # Markdown functions\n               \"ggplot2\",       # Data visualization\n               \"patchwork\"      # Patching plots together\n)",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#loading-data",
    "href": "pint_prd2.html#loading-data",
    "title": "PINT - Prediction Practical 2",
    "section": "3.2 Loading data",
    "text": "3.2 Loading data\nFor this practical, we will again use the Traumatic brain injury (TBI) data. This dataset contains 2,159 patients from the international and US Tirilazad trials (distributed here for didactic purposes only). The primary outcome was the Glasgow Outcome (range 1 through 5) at 6 months.\nThe TBI dataset was sent to you together with this practical and can be loaded from your local device:\n# Specify path of TBI dataset\npath &lt;- \"C:/users/avid_PINT_student/documents/pint/TBI.txt\"\n\n# Load TBI data\ntbi &lt;- import(path)\n\n\nWarning in (function (input = \"\", file = NULL, text = NULL, cmd = NULL, :\nDetected 24 column names but the data has 25 columns (i.e. invalid file). Added\n1 extra default column name for the first column which is guessed to be row\nnames or an index. Use setnames() afterwards if this guess is not correct, or\nfix the file write command that created the file to create a valid file.\n\n\nThe warning we receive is the result of row numbers being present in the .txt file (as also guessed by import()). Given that import() guessed correct, we can ignore this warning and treat the extra column V1 as individual identifiers.\nBelow is the codebook for the TBI data.\n\nCodebook for the TBI dataset\n\n\n\n\n\n\n\n\nTBI\n\n\n\nVariable\nDescription\n\n\n\nV1\nIdentifier\n\n\n\ntrial\nTrial identification\n\n\n\nd.gos\nGlasgow Outcome Scale at 6 months\n\ndead\nvegetative\nsevere disability\nmoderate disability\ngood recovery\n\n\n\n\nd.mort\nMortality at 6 months\n\n\n\nd.unfav\nUnfavourable outcome at 6 months\n\n\n\ncause\nCause of injury\n\nroad traffic accident\nmotorbike\nassault\ndomestic/fall\nother\n\n\n\n\nage\nAge (yrs)\n\n\n\nd.motor\nAdmission motor score (range 1-6)\n\n\n\nd.pupil\nPupillary reactivity\n\nboth reactive\none reactive\nnone reactive\n\n\n\n\npupil.i\nSingle imputed pupillary reactivity\n\n\n\nhypoxia\nHypoxia before or at admission\n\n\n\nhypotens\nHypotension before or at admission\n\n\n\nctclass\nMarshall CT classification (range 1-6)\n\n\n\ntsah\ntSAH at CT\n\n\n\nedh\nEDH at CT\n\n\n\ncisterns\nCompressed cisterns at CT\n\nno\nslightly\nfully\n\n\n\n\nshift\nMidline shift &gt;5 mm at CT\n\n\n\nd.sysbpt\nSystolic blood pressure at admission (mmHg)\n\n\nglucose\nGlucose at admission (mmol/L)\n\n\n\nglucoset\nTruncated glucose values (mmol/L)\n\n\n\nph\npH\n\n\n\nsodium\nSodium (mmol/L)\n\n\n\nhb\nHemoglobin (g/dL)\n\n\n\nhbt\nTruncated hemoglobin (g/dL)",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#preparing-our-data",
    "href": "pint_prd2.html#preparing-our-data",
    "title": "PINT - Prediction Practical 2",
    "section": "3.3 Preparing our data",
    "text": "3.3 Preparing our data\nLike last time, we will develop a prediction model for the risk of an unfavourable outcome after a TBI. However, we will geographically split the data into a development dataset (individuals from the Tirilazad US trial) and a validation dataset (individuals from the Tirilazad International trial). Additionally, although not recommended, for educational simplicity, we have again used single imputation for missing data.\n\n# Prior to split, set categorical variables to factors\ntbi %&lt;&gt;% mutate(across(c(pupil.i, cause), as.factor))\n\n# Create development dataset\ndevelopment &lt;- filter(tbi, trial == \"Tirilazad US\")\n\n# Create validation dataset\nvalidation &lt;- filter(tbi, trial == \"Tirilazad International\")",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#getting-to-know-our-data",
    "href": "pint_prd2.html#getting-to-know-our-data",
    "title": "PINT - Prediction Practical 2",
    "section": "3.4 Getting to know our data",
    "text": "3.4 Getting to know our data\nTo get to know our data, we will look at the number of cases in both datasets.\n\n# Counts of non-cases and cases\nby(tbi[[\"d.unfav\"]], tbi[[\"trial\"]], table)\n\ntbi[[\"trial\"]]: Tirilazad International\n\n  0   1 \n662 456 \n------------------------------------------------------------ \ntbi[[\"trial\"]]: Tirilazad US\n\n  0   1 \n646 395 \n\n# Distribution of non-cases and cases (as %)\nby(tbi[[\"d.unfav\"]], tbi[[\"trial\"]], \\(x) proportions(table(x)) * 100)\n\ntbi[[\"trial\"]]: Tirilazad International\nx\n       0        1 \n59.21288 40.78712 \n------------------------------------------------------------ \ntbi[[\"trial\"]]: Tirilazad US\nx\n       0        1 \n62.05572 37.94428 \n\n\nIn the development dataset, we can see that we have quite some cases (n = 395, 37.9%). We have a bit more in the validation dataset (n = 456, 40.8%).\nWe (hypothetically) consulted the same TBI experts as last time, who told us that the most important predictors we want to include are pupillary reactivity (‘pupil.i’), TBI cause (‘cause’), systolic blood pressure (d.sysbpt), and age (‘age’). They do not mention motor activity anymore due to recent research of them. This would mean that there are a total of 8 candidate coefficients. If we were to calculate an events-per-variable (EPV), this would be:\n\n# EPV in both datasets\nby(tbi[[\"d.unfav\"]], tbi[[\"trial\"]], \\(x) table(x)[[\"1\"]] / 8)\n\ntbi[[\"trial\"]]: Tirilazad International\n[1] 57\n------------------------------------------------------------ \ntbi[[\"trial\"]]: Tirilazad US\n[1] 49.375\n\n\nThis is not bad, but EPV is also not an ideal measure. More formal sample size calculations are available but are outside the scope of this course.\nLast time, we already saw that even though age is continuous, a linear term is sufficient to include it in the model. We also checked this for systolic blood pressure, which was also linear.",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#standard-model",
    "href": "pint_prd2.html#standard-model",
    "title": "PINT - Prediction Practical 2",
    "section": "4.1 Standard model",
    "text": "4.1 Standard model\nTo show how shrinkage affects the model, we will first develop a standard logistic prediction model.\n\n# Develop the prediction model\nfit &lt;- glm(d.unfav ~                                        \n               age +     \n               d.sysbpt +\n               pupil.i +\n               cause, \n           data = development, family = binomial)     \n\n# See output\nsummary(fit)\n\n\nCall:\nglm(formula = d.unfav ~ age + d.sysbpt + pupil.i + cause, family = binomial, \n    data = development)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                -1.382712   0.698858  -1.979   0.0479 *  \nage                         0.037291   0.005986   6.230 4.68e-10 ***\nd.sysbpt                   -0.008379   0.004562  -1.836   0.0663 .  \npupil.ino reactive pupils   1.836702   0.176985  10.378  &lt; 2e-16 ***\npupil.ione reactive         0.834370   0.206248   4.045 5.22e-05 ***\ncausedomestic/fall          0.296242   0.293463   1.009   0.3128    \ncauseMotorbike             -0.019630   0.309420  -0.063   0.9494    \ncauseother                  0.483429   0.283144   1.707   0.0878 .  \ncauseRoad traffic accident  0.266325   0.257151   1.036   0.3004    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1382.0  on 1040  degrees of freedom\nResidual deviance: 1200.2  on 1032  degrees of freedom\nAIC: 1218.2\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#ridge-regression",
    "href": "pint_prd2.html#ridge-regression",
    "title": "PINT - Prediction Practical 2",
    "section": "4.2 Ridge regression",
    "text": "4.2 Ridge regression\nRidge regression, also known as \\(l_2\\) regularization, applies a penalty to the maximum likelihood estimator, leading to shrinkage of the regression coefficients, which results in a less optimistic model. In the below formula, the white part shows the maximum likelihood estimator of a logsitic model and the red part the addition of the \\(l_2\\) regularization:\n\\[\n\\log{L} = \\sum_iy_i\\log{\\pi_i}+(1-y_i)\\log{(1-\\pi_i)}-\\color{blue}{\\lambda\\sum_{j=1}^P\\hat{\\beta}_j^2}\n\\]\nwhere \\(i\\) stands for individual \\(i\\), \\(y_i\\) is the observed outcome for individual \\(i\\) and \\(\\pi_i\\) is the predicted outcome for individual i.\nWe can fit a ridge regression using glmnet() from the {glmnet} package. However, first we require a value for \\(\\lambda\\). The preferred way to determine this is by \\(k\\)-fold cross-validation, as implemented in the function cv.glmnet()from {glmnet} package:\n\n1# Prepare predictors\nx &lt;- development %&gt;%\n    # Select predictors\n    select(age, d.sysbpt, pupil.i, cause) %&gt;%\n    # Change to matrix\n2    model.matrix(~ age + d.sysbpt + pupil.i + cause, data = .)\n\n# Remove intercept column from x (makes output easier to read)\nx &lt;- x[, -1]\n    \n\n3# Prepare outcome\ny &lt;- development %&gt;%\n    # Select outcome\n    select(d.unfav) %&gt;%\n    # Change to matrix\n    as.matrix()\n\n# Run cross-validation\ncv_ridge &lt;- cv.glmnet(x = x, \n                      y = y, \n                      family = \"binomial\",\n                      type.measure = \"mse\",\n4                      alpha = 0,\n5                      nfolds = 10)\n\n# Get optimal lambda\n6(lambda &lt;- cv_ridge[[\"lambda.min\"]])\n\n7# Show results of cross-validation\nggplot(mapping = aes(x = log(cv_ridge[[\"lambda\"]]), y = cv_ridge[[\"cvm\"]], \n                     ymin = cv_ridge[[\"cvlo\"]], ymax = cv_ridge[[\"cvup\"]])) +\n    # Geometries\n    geom_point(colour = \"#785EF0\") +\n    geom_errorbar(width = 0.2, colour = \"#785EF0\") +\n    geom_vline(xintercept = log(lambda), alpha = 0.3, linetype = \"dashed\") +\n    # Labels\n    xlab(expression(log(lambda))) + \n    ylab(\"Mean squared error\") +\n    # Aesthetics\n    theme_bw()\n\n\n1\n\nThe function cv.glmnet() requires a matrix with the predictors, where each row corresponds to an observation and each column a predictor.\n\n2\n\nFactors are not allowed in the x matrix, which means that we should already create dummy variables for the factors. The function model.matrix() does this for us.\n\n3\n\nThe outcome should also be given as a matrix or vector.\n\n4\n\nWe specify alpha = 0 to indicate that we want to use ridge regression. If we use alpha = 1, we indicate LASSO regression. Anything in between would result in elastic net regression.\n\n5\n\nWe specify 10-fold cross-validation.\n\n6\n\nThe optimal \\(\\lambda\\) is derived by taking lambda.min from the results.\n\n7\n\nWe can show that the \\(\\lambda\\) we selected (the dashed line) is the one with the lowest mean squared error. The x-axis is in log scale for clarity.\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.01576116\n\n\nNow we can fit our ridge regression model:\n\n# Fit ridge regression\nfit_ridge &lt;- glmnet(x = x,\n                    y = y,\n                    family = \"binomial\",\n                    alpha = 0,\n                    lambda = lambda)\n                    \n# Compare coefficients\ndata.frame(standard = fit[[\"coefficients\"]],\n           ridge = coef(fit_ridge)[1:9])\n\n                               standard        ridge\n(Intercept)                -1.382712131 -1.175577461\nage                         0.037291271  0.033647308\nd.sysbpt                   -0.008378515 -0.007997321\npupil.ino reactive pupils   1.836702419  1.681658780\npupil.ione reactive         0.834370082  0.747330320\ncausedomestic/fall          0.296242006  0.216124849\ncauseMotorbike             -0.019630030 -0.105348530\ncauseother                  0.483428921  0.370953923\ncauseRoad traffic accident  0.266325399  0.163070376\n\n\nComparing these coefficients to the standard model, we may observe that the coefficients are shrunk. If you have a polychotomous predictor, not all coefficients for that predictor might shrink.\nIf we set \\(\\lambda\\) to 0, we can see that it will be the same as the standard logistic regression:\n\n# Fit ridge regression with lambda 0\nfit_glmnet &lt;- glmnet(x = x,\n                     y = y,\n                     family = \"binomial\",\n                     alpha = 0,\n                     lambda = 0)\n\n# Compare coefficients\ndata.frame(glm = fit[[\"coefficients\"]],\n           glmnet = coef(fit_glmnet)[1:9])\n\n                                    glm      glmnet\n(Intercept)                -1.382712131 -1.38090517\nage                         0.037291271  0.03729148\nd.sysbpt                   -0.008378515 -0.00838120\npupil.ino reactive pupils   1.836702419  1.83677107\npupil.ione reactive         0.834370082  0.83440811\ncausedomestic/fall          0.296242006  0.29438080\ncauseMotorbike             -0.019630030 -0.02141978\ncauseother                  0.483428921  0.48173948\ncauseRoad traffic accident  0.266325399  0.26484164",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#lasso-regression",
    "href": "pint_prd2.html#lasso-regression",
    "title": "PINT - Prediction Practical 2",
    "section": "4.3 LASSO regression",
    "text": "4.3 LASSO regression\nThe least absolute shrinkage and selection operator (LASSO) regression functions similar to ridge regression, except that it is able to shrink coefficients to 0, leading to removal of those predictors from the prediction model. It applies the \\(l_1\\) penalty and is expressed as:\n\\[\n\\log{L} = \\sum_iy_i\\log{\\pi_i}+(1-y_i)\\log{(1-\\pi_i)}-\\color{red}{\\lambda\\sum_{j=1}^{P}\\lvert\\hat{\\beta}_j\\lvert}\n\\]\n\n# Run cross-validation\ncv_ridge &lt;- cv.glmnet(x = x, \n                      y = y, \n                      family = \"binomial\",\n                      type.measure = \"mse\",\n1                      alpha = 1,\n                      nfolds = 10) \n\n# Get optimal lambda\n(lambda &lt;- cv_ridge[[\"lambda.min\"]])         \n\n\n1\n\nNow we specify alpha = 1, indicating LASSO regression.\n\n\n\n\n[1] 0.008811767\n\n\nUsing our newly estimated \\(\\lambda\\), we can fit a LASSO regression:\n\n# Fit LASSO regression\nfit_lasso &lt;- glmnet(x = x,\n                    y = y,\n                    family = \"binomial\",\n                    alpha = 1,\n                    lambda = lambda)\n                    \n# Compare coefficients\ndata.frame(standard = fit[[\"coefficients\"]],\n           lasso = coef(fit_lasso)[1:9])\n\n                               standard        lasso\n(Intercept)                -1.382712131 -1.245073145\nage                         0.037291271  0.033261967\nd.sysbpt                   -0.008378515 -0.006202844\npupil.ino reactive pupils   1.836702419  1.698588282\npupil.ione reactive         0.834370082  0.687648609\ncausedomestic/fall          0.296242006  0.000000000\ncauseMotorbike             -0.019630030 -0.146980585\ncauseother                  0.483428921  0.152216095\ncauseRoad traffic accident  0.266325399  0.000000000\n\n\nFrom this output, we can see that two levels of cause were set to 0, effectively removing those variables from the final prediction model.",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#elastic-net-regression",
    "href": "pint_prd2.html#elastic-net-regression",
    "title": "PINT - Prediction Practical 2",
    "section": "4.4 Elastic net regression",
    "text": "4.4 Elastic net regression\nA special case is elastic net regression, where we combine the \\(l_1\\) and \\(l_2\\) penalties, or in other words blend ridge and LASSO regression. This takes the advantages of both approaches. A logistic elastic net model is expressed as\n\\[\n\\log{L} = \\sum_iy_i\\log{\\pi_i}+(1-y_i)\\log{(1-\\pi_i)}-\n\\color{green}{\\lambda}\\left(\n\\color{red}{\\alpha\\sum_{j=1}^{P}\\lvert\\hat{\\beta}_j\\lvert}+\n\\color{blue}{\\frac{1-\\alpha}{2}\\lambda\\sum_{p=1}^P\\hat{\\beta}_p^2}\\right)\n\\]\nWe can see that \\(\\lambda\\) applies to both penalties at the same time and that \\(\\alpha\\) determines to what extent each penalty is applied. For instance, we can perform an elastic net regression with an alpha of 0.5 as follows:\n\n# Fit LASSO regression\nfit_elnet &lt;- glmnet(x = x,\n                    y = y,\n                    family = \"binomial\",\n                    alpha = 0.5,\n                    lambda = lambda)\n                    \n# Compare coefficients\ndata.frame(standard = fit[[\"coefficients\"]],\n           `elastic net` = coef(fit_elnet)[1:9])\n\n                               standard  elastic.net\n(Intercept)                -1.382712131 -1.166262635\nage                         0.037291271  0.034033114\nd.sysbpt                   -0.008378515 -0.007323096\npupil.ino reactive pupils   1.836702419  1.725088065\npupil.ione reactive         0.834370082  0.738591204\ncausedomestic/fall          0.296242006  0.055738038\ncauseMotorbike             -0.019630030 -0.169522827\ncauseother                  0.483428921  0.224909450\ncauseRoad traffic accident  0.266325399  0.027031510\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that for ridge regression, LASSO regression, and elastic net regression, we need to estimate separate \\(\\lambda\\)’s.",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#performance",
    "href": "pint_prd2.html#performance",
    "title": "PINT - Prediction Practical 2",
    "section": "4.5 Performance",
    "text": "4.5 Performance\n\n4.5.1 Set-up for calculating validation measures\nNow that we fit our standard and shrunk models, we can see how they compare in performance, especially in an external validation set.\nWe will first create a function to calculate the C-statistic for our models:\n\n# Function to calculate C-statistic for output of a glmnet\ncstat &lt;- function(fit, x, y, df){\n    # Calculate predictions for glmnet output\n    if(\"glmnet\" %in% class(fit)) preds &lt;- predict(fit, newx = x, type = \"response\")\n    \n    # Calculate predictions for glm output\n    else preds &lt;- predict(fit, newdata = df, type = \"response\")\n    \n    # Calculate C-statistic\n    cst &lt;- roc(as.numeric(y), as.numeric(preds), levels = c(0, 1), direction = \"&lt;\")\n    \n    # Return c-statistic\n    return(cst[[\"auc\"]][[1]])\n}\n\n\n\nWe do not need to create these functions. However, to keep our script legible and more efficient, we pooled together the calculation of C-statistics for glm and glmnet objects. Simply, it calculates the predictions based on whether the model was fitted using glm() or glmnet() and then uses the roc() function from {pROC} to calculate the C-statistic and return it.\nWe will also need to create the x and y matrices of the validation data:\n\n# Prepare predictors                            \nx_val &lt;- validation %&gt;%\n    # Select predictors\n    select(age, d.sysbpt, pupil.i, cause) %&gt;%\n    # Change to matrix\n    model.matrix(~ age + d.sysbpt + pupil.i + cause, data = .)\n\n# Remove intercept column from x (makes output easier to read)\nx_val &lt;- x_val[, -1]\n    \n# Prepare outcome                                \ny_val &lt;- validation %&gt;%\n    # Select outcome\n    select(d.unfav) %&gt;%\n    # Change to matrix\n    as.matrix()\n\nMoreover, we will create a function to calculate the calibration intercept and slope for all models:\n\n# Function to calculate calibration metrics\ncalmetrics &lt;- function(fit, x = NULL, y = NULL, df = NULL){\n    # Calculate predictions for glmnet output\n    if(\"glmnet\" %in% class(fit)) preds &lt;- predict(fit, newx = x)\n    \n    # Calculate predictions for glm output\n    else preds &lt;- predict(fit, newdata = df)\n    \n    # Calculate calibration\n    fit_cal &lt;- glm(y ~ preds, family = binomial)\n    \n    # Get intercept and slope\n    results &lt;- list(intercept = round(fit_cal[[\"coefficients\"]][[1]], 3),\n                    slope = round(fit_cal[[\"coefficients\"]][[2]], 3))\n    \n    # Return results\n    return(results)\n}\n\nThe last thing we will do is putting all predictions in a single data frame to draw our calibration plots:\n\n# Function to calculate predictions\npreds &lt;- function(fit, x, y, df){\n    # Calculate predictions for glmnet output\n    if(\"glmnet\" %in% class(fit)) preds &lt;- predict(fit, newx = x, type = \"response\")\n    \n    # Calculate predictions for glm output\n    else preds &lt;- predict(fit, newdata = df, type = \"response\")\n    \n    # Return predictions\n    return(preds)\n}\n\n## Create data for each model\n# Standard model\ndat_standard &lt;- tibble(cohort = c(rep(\"Development\", nrow(development)), rep(\"Validation\", nrow(validation))),\n                       preds = c(preds(fit, y = y, df = development), preds(fit, y = y_val, df = validation)),\n                       obs = c(y, y_val)) %&gt;%\n    # Add indicator\n    mutate(model = \"Standard\")\n\n# Ridge model\ndat_ridge &lt;- tibble(cohort = c(rep(\"Development\", nrow(development)), rep(\"Validation\", nrow(validation))),\n                    preds = c(preds(fit_ridge, x, y), preds(fit_ridge, x_val, y_val)),\n                    obs = c(y, y_val))%&gt;%\n    # Add indicator\n    mutate(model = \"Ridge\")\n\n# LASSO model\ndat_lasso &lt;- tibble(cohort = c(rep(\"Development\", nrow(development)), rep(\"Validation\", nrow(validation))),\n                    preds = c(preds(fit_lasso, x, y), preds(fit_lasso, x_val, y_val)),\n                    obs = c(y, y_val))%&gt;%\n    # Add indicator\n    mutate(model = \"LASSO\")\n\n# Elastic net model\ndat_elnet &lt;- tibble(cohort = c(rep(\"Development\", nrow(development)), rep(\"Validation\", nrow(validation))),\n                    preds = c(preds(fit_elnet, x, y), preds(fit_elnet, x_val, y_val)),\n                    obs = c(y, y_val))%&gt;%\n    # Add indicator\n    mutate(model = \"Elastic net\")\n\n# Combine all data\ndat_cal &lt;- rbind(dat_standard,\n                 dat_ridge,\n                 dat_lasso,\n                 dat_elnet) %&gt;%\n    # Change models to factor\n    mutate(model = factor(model, levels = c(\"Standard\", \"Ridge\", \"LASSO\", \"Elastic net\")))\n\n\n\n4.5.2 Validate models\nNow we can determine how the different models performed.\n\n# Calculate C-statistics\ncstats &lt;- tibble(model = rep(c(\"standard\", \"ridge\", \"lasso\", \"elastic net\"), 2),\n                 cohort = c(rep(\"development\", 4), rep(\"validation\", 4)),\n                 cstat = c(cstat(fit, y = y, df = development), cstat(fit_ridge, x, y), \n                           cstat(fit_lasso, x, y), cstat(fit_elnet, x, y),\n                           cstat(fit, y = y_val, df = validation), cstat(fit_ridge, x_val, y_val), \n                           cstat(fit_lasso, x_val, y_val), cstat(fit_elnet, x_val, y_val)))\n\n# Pivot data\ncstats %&lt;&gt;% pivot_wider(names_from = cohort, values_from = cstat)\n\n# Print results\nkable(cstats)\n\n\n\n\nmodel\ndevelopment\nvalidation\n\n\n\n\nstandard\n0.7385586\n0.7011399\n\n\nridge\n0.7380766\n0.7012724\n\n\nlasso\n0.7365521\n0.7021469\n\n\nelastic net\n0.7369440\n0.7023059\n\n\n\n\n\nLet’s also look at the calibration metrics\n\n# Calculate C-statistics\ncalibm &lt;- tibble(model = rep(c(\"standard\", \"ridge\", \"lasso\", \"elastic net\"), 2),\n                 cohort = c(rep(\"development\", 4), rep(\"validation\", 4)),\n                 intercept = c(calmetrics(fit, y = y, df = development)[[1]], calmetrics(fit_ridge, x, y)[[1]], \n                               calmetrics(fit_lasso, x, y)[[1]], calmetrics(fit_elnet, x, y)[[1]],\n                               calmetrics(fit, y = y_val, df = validation)[[1]], calmetrics(fit_ridge, x_val, y_val)[[1]], \n                               calmetrics(fit_lasso, x_val, y_val)[[1]], calmetrics(fit_elnet, x_val, y_val)[[1]]),\n                 slope = c(calmetrics(fit, y = y, df = development)[[2]], calmetrics(fit_ridge, x, y)[[2]], \n                           calmetrics(fit_lasso, x, y)[[2]], calmetrics(fit_elnet, x, y)[[2]],\n                           calmetrics(fit, y = y_val, df = validation)[[2]], calmetrics(fit_ridge, x_val, y_val)[[2]], \n                           calmetrics(fit_lasso, x_val, y_val)[[2]], calmetrics(fit_elnet, x_val, y_val)[[2]]))\n\n# Pivot data\ncalibm %&lt;&gt;% pivot_wider(names_from = cohort, values_from = c(intercept, slope))\n\n# Print calibration metrics \nkable(calibm)\n\n\n\n\n\n\n\n\n\n\n\nmodel\nintercept_development\nintercept_validation\nslope_development\nslope_validation\n\n\n\n\nstandard\n0.000\n0.154\n1.000\n0.875\n\n\nridge\n0.046\n0.199\n1.096\n0.959\n\n\nlasso\n0.054\n0.235\n1.113\n1.004\n\n\nelastic net\n0.041\n0.212\n1.085\n0.967\n\n\n\n\n\nWe can also look at the calibration plots:\n\n# Create calibration plots for each model\nggplot(dat_cal, mapping = aes(x = preds, y = obs, colour = cohort)) +               \n    # Geometries\n    geom_abline(linewidth = 1, colour = \"black\", alpha = 0.33) +   \n    geom_point(alpha = 0.33) +  \n    geom_smooth(colour = \"#785EF0\", fill = \"#785EF0\", method = \"loess\", formula = \"y ~ x\") +  \n    # Scaling\n    scale_colour_manual(values = c(\"#648FFF\", \"#FFB000\"), aesthetics = c(\"fill\", \"colour\")) +\n    # Labels                              \n    xlab(\"Predicted probability\") +            \n    ylab(\"Observed probability\") +\n    # Transformations\n    coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +\n    facet_grid(rows = vars(model), cols = vars(cohort)) +\n    # Aesthetics                            \n    theme_bw() +\n    theme(legend.title = element_blank(),\n          legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhat we see is that (although small for the C-statistics), the standard model performs better in the development data, but worse in the validation data. Note however that this does not have to be the case: van Calster et al..",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#development",
    "href": "pint_prd2.html#development",
    "title": "PINT - Prediction Practical 2",
    "section": "5.1 Development",
    "text": "5.1 Development\nIn short, we develop the prediction model on our full development data. Then, we bootstrap the data \\(B\\) times and redevelop the model in each bootstrap sample \\(b\\). Each redeveloped model is applied to the development data and a calibration slope is calculated. Then, the shrinkage factor \\(s\\) equals the average calibration slope among all bootstrap samples. The final regression coefficients are then calculated as \\(\\hat{\\beta}_{j,shrunk} = \\hat{\\beta}_j * s\\).\nWe already developed our standard model. For the bootstrapping, we can do:\n\n# Specify number of bootstraps\nb &lt;- 500\n\n# Start bootstrapping procedure\n1s &lt;- sapply(1:b, \\(x){\n    # Set seed based on bootstrap iteration\n2    set.seed(x)\n    \n    # Randomly sample data with replacement\n    dat_smp &lt;- slice_sample(development,  \n3                            n = nrow(development),\n4                            replace = TRUE)\n    \n    # Redevelop model\n    fit_smp &lt;- glm(d.unfav ~                                        \n                       age +     \n                       d.sysbpt +\n                       pupil.i +\n                       cause, \n5                   data = dat_smp, family = binomial)\n    \n    # Get linear predictors\n6    development[[\"lps\"]] &lt;- predict(fit_smp, newdata = development)\n    \n    # Calculate calibration slope\n7    slope &lt;- glm(d.unfav ~ lps, data = development, family = binomial)[[\"coefficients\"]][[\"lps\"]]\n    \n    # Return slope\n    return(slope)\n}) %&gt;%\n    # Take mean\n8    mean()\n\n\n1\n\nStart bootstrapping procedure, where we iterate 500 times (b) and call a function (\\(x)).\n\n2\n\nSet a seed for the random process each bootstrap (the sampling)\n\n3\n\nSample as many individuals as our data contains.\n\n4\n\nSample is with replacement, meaning the same individual can be sampled multiple times.\n\n5\n\nRedevelop the model on the sampled data.\n\n6\n\nMake predictions on the development data using the sample-based prediction model.\n\n7\n\nCalculate the slope.\n\n8\n\nTake the mean of the slopes from all bootstraps.\n\n\n\n\nNow that we have \\(s\\), we can multiply it with the coefficients of the standard model:\n\n# Take coefficients from standard model and multiply by s\ncoefs &lt;- fit[[\"coefficients\"]] * s\n\n# Compare coefficients\ndata.frame(standard = fit[[\"coefficients\"]],\n           bu = coefs)\n\n                               standard           bu\n(Intercept)                -1.382712131 -1.323924298\nage                         0.037291271  0.035705784\nd.sysbpt                   -0.008378515 -0.008022292\npupil.ino reactive pupils   1.836702419  1.758612589\npupil.ione reactive         0.834370082  0.798895735\ncausedomestic/fall          0.296242006  0.283646886\ncauseMotorbike             -0.019630030 -0.018795434\ncauseother                  0.483428921  0.462875302\ncauseRoad traffic accident  0.266325399  0.255002223",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd2.html#validation",
    "href": "pint_prd2.html#validation",
    "title": "PINT - Prediction Practical 2",
    "section": "5.2 Validation",
    "text": "5.2 Validation\nPrior to validating the model, we will wrangle the existing model fit a bit so that it contains the information relevant for validating the model.\n\n# Update fit with new coefficients\nfit_bu &lt;- fit\n\n# Add updated coefficients\nfit_bu[[\"coefficients\"]] &lt;- coefs\n\n# Recalculate linear predictors\n# We perform matrix multiplication of each individual's values with the coefficients minus the intercept (as the matrix x does not contain this). Afterwards, we add the intercept again.\nfit_bu[[\"linear.predictors\"]] &lt;- x %*% as.matrix(fit_bu[[\"coefficients\"]])[2:9] + fit_bu[[\"coefficients\"]][[1]]\n\n# Recalculate the predicted risk\nfit_bu[[\"fitted.values\"]] &lt;- exp(fit_bu[[\"linear.predictors\"]]) / (1 + exp(fit_bu[[\"linear.predictors\"]]))\n\nThis may seem like a bit of a hassle, especially since the {rms} package already implements this. On the other hand, as discussed in the previous practical, using the {rms} package might have its downsides.\nNow let’s see the validation metrics:\n\ntibble(model = c(\"standard\", \"BU\"),\n       cstat_dev = c(cstat(fit, y = y, df = development), cstat(fit_bu, y = y, df = development)),\n       cstat_val = c(cstat(fit, y = y_val, df = validation), cstat(fit_bu, y = y_val, df = validation)),\n       intercept_dev = c(calmetrics(fit, y = y, df = development)[[1]], calmetrics(fit_bu, y = y, df = development)[[1]]),\n       intercept_val = c(calmetrics(fit, y = y_val, df = validation)[[1]], calmetrics(fit_bu, y = y_val, df = validation)[[1]]),\n       slope_dev = c(calmetrics(fit, y = y, df = development)[[2]], calmetrics(fit_bu, y = y, df = development)[[2]]),\n       slope_val = c(calmetrics(fit, y = y_val, df = validation)[[2]], calmetrics(fit_bu, y = y_val, df = validation)[[2]]))\n\n# A tibble: 2 × 7\n  model    cstat_dev cstat_val intercept_dev intercept_val slope_dev slope_val\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 standard     0.739     0.701             0         0.154      1        0.875\n2 BU           0.739     0.701             0         0.154      1.04     0.914\n\n\nWe see that the C-statistics remained the same, likely because the shift in final predictions was small. However, the calibration of the standard model was better than the bootstrapped model in the development data, but worse in the validation data, as expected.",
    "crumbs": [
      "PINT practical 2"
    ]
  },
  {
    "objectID": "pint_prd1.html",
    "href": "pint_prd1.html",
    "title": "PINT - Prediction Practical 1",
    "section": "",
    "text": "This R practical will discuss the necessities for prediction modelling in R. However, we assume some basic knowledge on how R works. If you are new to R, we suggest you take a look at this freely available R tutorial. Following the tutorial up until the section ‘dplyr’ should be enough to understand this practical.",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#getting-to-know-our-data",
    "href": "pint_prd1.html#getting-to-know-our-data",
    "title": "PINT - Prediction Practical 1",
    "section": "5.1 Getting to know our data",
    "text": "5.1 Getting to know our data\nThe degree to which a prediction model is likely not overfitted depends in part on the number of cases and non-cases:\n\n# Counts of non-cases and cases\ntable(tbi[[\"d.unfav\"]])\n\n\n   0    1 \n1308  851 \n\n# Distribution of non-cases and cases (as %)\nproportions(table(tbi[[\"d.unfav\"]])) * 100\n\n\n      0       1 \n60.5836 39.4164 \n\n\nIn the TBI dataset, we can see that we have quite a lot of cases (n = 851, 39.4%).\nWe (hypothetically) consulted some TBI experts, who told us that the most important predictors we want to include are motor activity (‘d.motor’), pupillary reactivity (‘d.pupil’), TBI cause (‘cause’), and age (‘age’). Although we will estimate quite a lot of coefficients (12) because motor activity, pupillary reactivity, and cause are categorical, we also have quite some events so we are not too worried about overfitting.\nBecause age is continuous, we should investigate whether adding it as a linear term is sufficient or if we model it non-linearly. We can do this by modelling the outcome as a function of age and visually inspecting the results.\n\n# Model the outcome as a function of age\n1fit &lt;- glm(d.unfav ~ age, data = tbi, family = binomial)\n\n# Plot predicted probabilities vs. age\n2ggplot(data = tbi, aes(x = age, y = fit[[\"fitted.values\"]])) +\n    # Geometries\n3    geom_point() +\n    # Labels\n4    xlab(\"Age (yrs)\") + ylab(\"Predicted probability\") +\n    # Aesthetics\n5    theme_bw()\n\n\n1\n\nFit a logistic regression model with the outcome ‘d.unfav’ as a function of age.\n\n2\n\nCreate the basis for a plot, where age is on the x-axis and the fitted values from the logistic regression model are on the y-axis.\n\n3\n\nDraw points in the plot based on the data.\n\n4\n\nChange the axes labels.\n\n5\n\nChange the look of the plot to a pre-specified theme.\n\n\n\n\n\n\n\n\n\n\n\nFrom the figure, we can see that age is linearly associated with the outcome, so we do not need to apply any further transformation.",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#method-i-glm",
    "href": "pint_prd1.html#method-i-glm",
    "title": "PINT - Prediction Practical 1",
    "section": "5.2 Method I: glm()",
    "text": "5.2 Method I: glm()\n\n5.2.1 Developing the prediction model\nWe can now develop our prediction model. Because the outcome is binary, we will use a logistic regression model. Because a logistic regression model is part of the generalized linear models (GLM) family, we can use the glm() function as already done above. We specify that the family (i.e. the distribution of the data) is binomial (0 or 1), which specifies that we are fitting a logistic model.\n\n# Develop the prediction model\n1fit &lt;- glm(d.unfav ~\n2               age +\n3               as.factor(d.motor) +\n               as.factor(d.pupil) +\n               as.factor(cause), \n4           data = tbi, family = binomial)\n\n\n1\n\nFit a logistic regression model with the outcome ‘d.unfav’.\n\n2\n\nAdd age as a normal predictor (because we saw it was linearly associated with the outcome).\n\n3\n\nAdd the categorical predictors as factors (to make sure R recognizes them as categories).\n\n4\n\nSpecify that the data source and the family to define a logistic model.\n\n\n\n\nLet’s see what the output looks like:\n\n# Print output\nsummary(fit)\n\n\nCall:\nglm(formula = d.unfav ~ age + as.factor(d.motor) + as.factor(d.pupil) + \n    as.factor(cause), family = binomial, data = tbi)\n\nCoefficients:\n                                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                           -1.633692   0.660303  -2.474 0.013355 *  \nage                                    0.037503   0.004037   9.289  &lt; 2e-16 ***\nas.factor(d.motor)2                    0.648468   0.621264   1.044 0.296583    \nas.factor(d.motor)3                    0.057519   0.614843   0.094 0.925466    \nas.factor(d.motor)4                   -0.660139   0.609902  -1.082 0.279089    \nas.factor(d.motor)5                   -1.304401   0.610827  -2.135 0.032723 *  \nas.factor(d.motor)6                   -1.628588   0.680168  -2.394 0.016648 *  \nas.factor(d.pupil)no reactive pupils   1.285761   0.148476   8.660  &lt; 2e-16 ***\nas.factor(d.pupil)one reactive         0.552311   0.148264   3.725 0.000195 ***\nas.factor(cause)domestic/fall          0.326742   0.247825   1.318 0.187357    \nas.factor(cause)Motorbike              0.162727   0.245626   0.662 0.507652    \nas.factor(cause)other                  0.435268   0.245729   1.771 0.076505 .  \nas.factor(cause)Road traffic accident  0.183664   0.231363   0.794 0.427292    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2726.7  on 2035  degrees of freedom\nResidual deviance: 2251.5  on 2023  degrees of freedom\n  (123 observations deleted due to missingness)\nAIC: 2277.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe attentive reader might notice that in one of the last lines, the following message is printed:\n\n\n(123 observations deleted due to missingness)\n\n\nIt is important to realize that, like much statistical software, individuals with missing values are excluded by default. However, if the missingness is informative (which is often the case), this likely results in a biased prediction model (and we lose some power). Luckily, the TBI data is supplied with the variable ‘pupil.i’. This is a single imputed version of ‘d.pupil’. Although single imputation is often unsatisfactory, for our current didactic purposes it is enough.\nIf we refit the model with the imputed variable, we get:\n\n# Develop the prediction model\nfit &lt;- glm(d.unfav ~\n               age +\n               as.factor(d.motor) +\n               as.factor(pupil.i) +\n               as.factor(cause), \n           data = tbi, family = binomial)\n\n# Print output\nsummary(fit)\n\n\nCall:\nglm(formula = d.unfav ~ age + as.factor(d.motor) + as.factor(pupil.i) + \n    as.factor(cause), family = binomial, data = tbi)\n\nCoefficients:\n                                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                           -1.462684   0.643773  -2.272  0.02308 *  \nage                                    0.037715   0.003887   9.703  &lt; 2e-16 ***\nas.factor(d.motor)2                    0.482682   0.607106   0.795  0.42658    \nas.factor(d.motor)3                   -0.068894   0.601206  -0.115  0.90877    \nas.factor(d.motor)4                   -0.739708   0.596682  -1.240  0.21509    \nas.factor(d.motor)5                   -1.378086   0.597590  -2.306  0.02111 *  \nas.factor(d.motor)6                   -1.784168   0.667584  -2.673  0.00753 ** \nas.factor(pupil.i)no reactive pupils   1.270588   0.142312   8.928  &lt; 2e-16 ***\nas.factor(pupil.i)one reactive         0.578122   0.142276   4.063 4.84e-05 ***\nas.factor(cause)domestic/fall          0.182092   0.235049   0.775  0.43852    \nas.factor(cause)Motorbike              0.075287   0.233211   0.323  0.74683    \nas.factor(cause)other                  0.356963   0.232921   1.533  0.12539    \nas.factor(cause)Road traffic accident  0.117619   0.219094   0.537  0.59138    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2895.5  on 2158  degrees of freedom\nResidual deviance: 2410.3  on 2146  degrees of freedom\nAIC: 2436.3\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe message on observations removed due to missingness is now gone (because no individuals were removed).\n\n\n5.2.2 Validating the prediction model\nNow that we have a prediction model, we have to make sure it actually works. To do this, we compare the observed and the predicted values. To this end, we will have to somehow get the predicted values from the prediction model.\nThere are two easy ways to do that:\n\n# Get predicted values from model fit object\n1tbi[[\"preds\"]] &lt;- fit[[\"fitted.values\"]]\n\n# Get predicted values from function\n2preds_fun &lt;- predict(fit, type = \"response\")\n\n# Compare values\n3table(tbi[[\"preds\"]] == preds_fun)\n\n\n1\n\nThe predicted values are already stored in the model fit object. This makes it easy for us to extract and use those values.\n\n2\n\nWe can also use the predict function. The predict function contains many subclasses that recognize what type of model we fitted. In this case, R automatically detects it should use the subclass predict.glm(). By defining ‘response’, we tell R that we want the predicted probabilities. The advantage of predict() is that we can also supply the newdata argument. By supplying newdata with a data frame with column names corresponding to the predictors, the predict() function calculates predictions for this new data.\n\n3\n\nWe can see that the predictions are the same for each individual.\n\n\n\n\n\nTRUE \n2159 \n\n\nWith these predictions, we can calculate our C-statistic:\n\n# Calculate C-statistic with the Cstat() function from the {DescTools} package.\nCstat(fit)                              \n\n[1] 0.7651261\n\n\nTo calculate our calibration intercept and calibration slope, we will need the linear predictors.\n\n# Get linear predictors from model fit for calibration intercept and slope\n1tbi[[\"lps\"]] &lt;- fit[[\"linear.predictors\"]]\n\n# Calculate calibration intercept and slope \n2fit_lps &lt;- glm(d.unfav ~ lps, data = tbi, family = binomial)\n\n# Get intercept and slope\n3cat(\"Intercept:\", format(round(fit_lps[[\"coefficients\"]][[\"(Intercept)\"]], 3), nsmall = 3))\ncat(\"\\nSlope:\", format(round(fit_lps[[\"coefficients\"]][[\"lps\"]], 3), nsmall = 3))\n\n\n1\n\nAs with getting the predicted values, we can use the model fit, or use the predict() function with type = \"link\", which has the advantage of allowing new data.\n\n2\n\nWe refit the regression model used to develop our prediction model with the outcome as a function of the linear predictors.\n\n3\n\nPrint the intercept and the slope (i.e. the regression coefficient for the linear predictors). round() rounds the values and format() prints the decimals up to the value supplied in nsmall, even if they are only 0’s.\n\n\n\n\nIntercept: 0.000\nSlope: 1.000\n\n\nFinally, we can make our calibration plot:\n\n# Create calibration plot\n1ggplot(tbi, aes(x = preds, y = d.unfav)) +\n    # Geometries\n2    geom_abline(linewidth = 1, colour = \"black\", alpha = 0.33) +\n3    geom_point(alpha = 0.33) +\n4    geom_smooth(colour = \"#785EF0\", fill = \"#785EF0\", method = \"loess\", formula = \"y ~ x\") +\n5    # Labels\n    xlab(\"Predicted probability\") +            \n    ylab(\"Observed probability\") +\n    # Aesthetics                            \n6    theme_bw()\n\n\n1\n\nWe create a base layer with the data for the calibration plot.\n\n2\n\nWe add the harmonisation line that shows perfect calibration, which is 67% transparent (alpha = 0.33).\n\n3\n\nWe draw points on the plot that represent the predicted vs. observed values for each individual.\n\n4\n\nBecause we have this data, we can also draw a locally estimated scatterplot smoothing (LOESS) curve which shows the calibration over the range of predicted values.\n\n5\n\nWe define the labels for the axes.\n\n6\n\nWe change the look of the plot based on a prespecified theme.\n\n\n\n\n\n\n\n\n\n\n\nWe can also add the histogram of predicted probabilities using ggMarginal() from {ggExtras}.\n\n# Create calibration plot\n1p &lt;- ggplot(tbi, aes(x = preds, y = d.unfav, colour = as.factor(d.unfav))) +\n    # Geometries\n    geom_abline(linewidth = 1, colour = \"black\", alpha = 0.33) +   \n3    geom_point(alpha = 0.33) +\n    geom_smooth(colour = \"#785EF0\", fill = \"#785EF0\", method = \"loess\", formula = \"y ~ x\") +  \n    # Scaling\n2    scale_colour_manual(values = c(\"#648FFF\", \"#FFB000\"),\n                        labels = c(\"Favourable outcome\", \"Unfavourable outcome\")) +\n    # Labels                               \n    xlab(\"Predicted probability\") +            \n    ylab(\"Observed probability\") +\n    # Aesthetics                            \n    theme_bw() +\n    theme(legend.position = \"bottom\",\n          legend.title = element_blank())  \n\n# Add histograms\n4ggMarginal(p, type = \"histogram\", margins = \"x\", binwidth = 0.01, groupFill = TRUE)\n\n\n1\n\nTo have two separately coloured histograms for individuals with and without the outcome, we need to specify on what variable to stratify colour. As a side effect, this also colours the points drawn with geom_point().\n\n2\n\nWith a manual scale, we define the colours and labels corresponding to each colour.\n\n3\n\nWe slightly adjust the theme by moving the legend to the bottom of the plot and removing the title of the legend.\n\n4\n\nWe add the histograms to the plot.",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#method-ii-lrm",
    "href": "pint_prd1.html#method-ii-lrm",
    "title": "PINT - Prediction Practical 1",
    "section": "5.3 Method II: lrm()",
    "text": "5.3 Method II: lrm()\nWe now used the glm() function from one of the base R packages ({stats}). However, there is another package that is often used to develop prediction models: {rms}.\n\n5.3.1 Developing the prediction model\nTo develop our prediction model using {rms}, we use the function lrm() for a logistic regression model.\n\n# First, set numeric variables to factor (lrm() does not work with as.factor())\ntbi[[\"d.motor\"]] &lt;- as.factor(tbi[[\"d.motor\"]])\n\n# Develop the prediction model\nfit_rms &lt;- lrm(d.unfav ~                        \n                   age +                        \n                   d.motor +         \n                   pupil.i +\n                   cause, \n1               data = tbi, x = TRUE, y = TRUE)\n\n\n1\n\nWe have to specify x and y as TRUE to use the functions for validation in {rms}.\n\n\n\n\n\n\n5.3.2 Validating the prediction model\nThe {rms} package offers an easy function validate prediction models: validate().\n\n# Validate the model\nvalidate(fit_rms)\n\n          index.orig training   test optimism index.corrected  n\nDxy           0.5303   0.5320 0.5251   0.0069          0.5233 40\nR2            0.2726   0.2754 0.2670   0.0083          0.2642 40\nIntercept     0.0000   0.0000 0.0013  -0.0013          0.0013 40\nSlope         1.0000   1.0000 0.9789   0.0211          0.9789 40\nEmax          0.0000   0.0000 0.0050   0.0050          0.0050 40\nD             0.2243   0.2269 0.2192   0.0077          0.2166 40\nU            -0.0009  -0.0009 0.0003  -0.0012          0.0003 40\nQ             0.2252   0.2278 0.2189   0.0089          0.2163 40\nB             0.1879   0.1866 0.1891  -0.0024          0.1903 40\ng             1.2470   1.2552 1.2267   0.0285          1.2185 40\ngp            0.2538   0.2539 0.2507   0.0032          0.2506 40\n\n\nHow do we interpret this output? The validate() function applies resampling, which is a method to account for overfitting, with the results being printed in the ‘index.corrected’ column. However, if we are just interested in the apparent performance, we can just look at ‘index.orig’. To calculate the C-statistic, we can use \\(c = \\frac{Dxy}{2} + 0.5\\). In our case, the C-statistic would be:\n\n# Calculate C-statistic\n0.5390 / 2 + 0.5\n\n[1] 0.7695\n\n\nThe intercept and slope are given under ‘Intercept’ and ‘Slope’. To get the calibration plot, we can use the calibrate() function (which aims to give overfitting-corrected estimates):\n\n# Get calibration data\nplot(calibrate(fit_rms))\n\n\n\n\n\n\n\n\n\nn=2159   Mean absolute error=0.006   Mean squared error=5e-05\n0.9 Quantile of absolute error=0.013",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#method-i-or-method-ii",
    "href": "pint_prd1.html#method-i-or-method-ii",
    "title": "PINT - Prediction Practical 1",
    "section": "5.4 Method I or Method II?",
    "text": "5.4 Method I or Method II?\nSo why would we pick one method over the other? Well, both methods have their advantages and disadvantages. It may be clear that {rms} has more native (i.e. inherent in the package) support for prediction modelling in the terms of validation with functions such as validate() and calibate(). Additionally, it makes it easy to get optimism-corrected performance measures. However, {rms} forces you to keep using the functions from its package, which are not always user intuitive. Moreover, only a few packages have been created as add-ons to {rms}, which makes it difficult to integrate into other analyses, or analyses that were not implemented/supported by the author. Conversely, glm() is part of the {stats} package, one of the base packages of R. A huge amount of packages have been developed which built on results from glm(). Additionally, as a base function of R, there is more focus on intuitive use and error-proofing.",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#the-curse-of-the-model-fit-object",
    "href": "pint_prd1.html#the-curse-of-the-model-fit-object",
    "title": "PINT - Prediction Practical 1",
    "section": "5.5 The curse of the model fit object",
    "text": "5.5 The curse of the model fit object\nWhether we develop a prediction model using glm() or lrm(), functions such as predict(), validate(), and calibrate(), require us to supply the model fit object: the R object which contains the fitted model. This makes it much easier for the developer of the prediction model (who has the model fit object) to determine performance. However, validation becomes much more difficult. Either the validator needs to be supplied with the model fit object (meaning a certain level of skill in the corresponding programming language of the model fit object is required), or the validator needs to use functions that do not require the model fit.\nBecause the model fit object is often not supplied, the validator has to resot to the functions that do not require the model fit. For the C-statistic, this could be roc() from the {pROC} package. However, to get the linear predictors, the modeller could use predict(), but the validator will have to calculate these based on the coefficients. This can mean a lot of (unnecessary) manual work, including the risk of error when writing down the predictor coefficients.",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#getting-to-know-our-data-1",
    "href": "pint_prd1.html#getting-to-know-our-data-1",
    "title": "PINT - Prediction Practical 1",
    "section": "6.1 Getting to know our data",
    "text": "6.1 Getting to know our data\nWe (again hypothetically) consulted some lung cancer experts, who told us that the most important predictors we want to include are age (‘age’), biological sex (‘sex’), and weight loss in the last six months (‘wt.loss’) and that we should predict death within two years. Because weight loss had some missings, we used single imputation. Again, given the didactic purpose of this practical, this is fine, but in a real research setting, multiple imputation should be used.\n\n# Single imputation of dat\nlung &lt;- complete(mice(lung, m = 1))\n\n\n iter imp variable\n  1   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n  2   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n  3   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n  4   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n  5   1  inst  ph.ecog  ph.karno  pat.karno  meal.cal  wt.loss\n\n\nAdditionally, some survival times go beyond two years, so let’s cap those survival times.\n\n# Cap survival times to two years\nlung &lt;- lung %&gt;%\n    # Change variables\n    mutate(# Set status to censored if we observed individuals longer than two years \n1           status = ifelse(time &gt; 365.25 * 2, 1, status),\n           # Cap time to two years\n2           time = ifelse(time &gt; 365.25 * 2, 365.25 * 2, time))\n\n\n1\n\nIf individuals died after two years, we did not observe an event within two years, therefore they become censored. Two years is defined by 365.25 * 2. ::: {.column-margin} Note that we use 365.25 days in a year. Although formally this should be 365.24, the second decimal is negligible for most follow-up durations in medical studies. :::\n\n2\n\nWe change the time until event to a maximum of two years. If the time is already shorter than two years, we keep that time.\n\n\n\n\nThe degree to which a prediction model is likely not overfitted depends in part on the number of cases and non-cases:\n\n# First, recode status to 0 and 1\nlung[[\"status\"]] &lt;- lung[[\"status\"]] - 1\n\n# Counts of non-cases and cases\ntable(lung[[\"status\"]])\n\n\n  0   1 \n 69 159 \n\n# Distribution of non-cases and cases (as %)\nproportions(table(lung[[\"status\"]])) * 100\n\n\n       0        1 \n30.26316 69.73684 \n\n\nIn the lung dataset, we can see that we have more cases than non-cases. In this case, we look at the number of non-cases to get an idea of the risk of overfitting. Given the already small sample size and the small number of non-cases (n = 69, 30.3%), we likely have a high risk of overfitting. Given the scope of this practical, we will leave that be for now, but recommend that in real research settings, you evaluate this risk and carefully choose a path forward.\nBecause age and weight loss are continuous, we should investigate whether adding them as a linear term is sufficient or if we should model them non-linearly. Instead of the predicted probability, we plot the log relative hazard as a function of the predictor, using a function from {Hmisc}, which combines functions from {rms} and {survival}.\n\n# Set seed for random process\nset.seed(1)\n\n# Model the outcome as a function of age\ndat_plot &lt;- do.call(\"cbind\", rcspline.plot(x = lung[[\"age\"]], y = lung[[\"time\"]], noprint = TRUE,\n                                           event = lung[[\"status\"]], model = \"cox\", statloc = \"none\")) %&gt;%\n    # Change to data frame\n    as.data.frame()\n\nAlthough the function already gives us a plot, the author of this practical is of the opinion that it is quite an ugly plot, so we will plot it ourselves:\n\n# Get plot\nggplot(dat_plot, aes(x = x, y = V3)) +\n    # Geometries\n    geom_ribbon(aes(ymin = V4, ymax = V5), fill = \"#785EF0\", alpha = 0.2) +\n    geom_line(colour = \"#785EF0\") +\n    # Scaling\n    scale_y_continuous(limits = c(min(dat_plot[[\"V4\"]]) - 1, max(dat_plot[[\"V5\"]]) + 1), \n                       name = \"Log relative hazard\") +\n    # Labelling\n    xlab(\"Age (yrs)\") +\n    # Aesthetics\n    theme_bw()\n\n\n\n\n\n\n\n\nNow we repeat this for weight loss.\n\n# Set seed for random process\nset.seed(1)\n\n# Model the outcome as a function of age\ndat_plot &lt;- do.call(\"cbind\", rcspline.plot(x = lung[[\"wt.loss\"]], y = lung[[\"time\"]], noprint = TRUE,\n                                           event = lung[[\"status\"]], model = \"cox\", statloc = \"none\")) %&gt;%\n    # Change to data frame\n    as.data.frame()\n\n\n# Get plot\nggplot(dat_plot, aes(x = x, y = V3)) +\n    # Geometries\n    geom_ribbon(aes(ymin = V4, ymax = V5), fill = \"#785EF0\", alpha = 0.2) +\n    geom_line(colour = \"#785EF0\") +\n    # Scaling\n    scale_y_continuous(limits = c(min(dat_plot[[\"V4\"]]) - 1, max(dat_plot[[\"V5\"]]) + 1), \n                       name = \"Log relative hazard\") +\n    # Labelling\n    xlab(\"Weight loss in last 6 months (pounds)\") +\n    # Aesthetics\n    theme_bw()\n\n\n\n\n\n\n\n\nAlthough for age a linear association is fine, we will need some kind of non-linear transformation for weight loss to accurately capture it in the prediction model.",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#method-i-coxph",
    "href": "pint_prd1.html#method-i-coxph",
    "title": "PINT - Prediction Practical 1",
    "section": "6.2 Method I: coxph()",
    "text": "6.2 Method I: coxph()\nTo develop the prediction model, we will use the coxph() function from the {survival} package. This works just like glm() in how we specify the prediction model, except that we specify the outcome using the Surv() function.\n\n# Develop the prediction model\n1fit &lt;- coxph(Surv(time, status) ~\n2               age +\n               sex +\n3               ns(wt.loss, knots = c(1, 12)),\n           data = lung)                       \n\n\n1\n\nFit a Cox regression model, with the outcome being a survival object containing the event and time to event.\n\n2\n\nAdd predictors age (linearly) and sex.\n\n3\n\nAdd weight loss as a natural cubic spline using the ns() function from {splines}. We put knots at 1 and 12 based on the previous graph.\n\n\n\n\nLet’s see what the output looks like:\n\n# Print output\nsummary(fit)\n\nCall:\ncoxph(formula = Surv(time, status) ~ age + sex + ns(wt.loss, \n    knots = c(1, 12)), data = lung)\n\n  n= 228, number of events= 159 \n\n                                    coef exp(coef)  se(coef)      z Pr(&gt;|z|)   \nage                             0.017152  1.017300  0.009474  1.811  0.07022 . \nsex                            -0.539257  0.583182  0.171639 -3.142  0.00168 **\nns(wt.loss, knots = c(1, 12))1 -0.217009  0.804922  0.499500 -0.434  0.66396   \nns(wt.loss, knots = c(1, 12))2 -2.970555  0.051275  1.676074 -1.772  0.07634 . \nns(wt.loss, knots = c(1, 12))3 -1.248597  0.286907  0.947628 -1.318  0.18764   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                               exp(coef) exp(-coef) lower .95 upper .95\nage                              1.01730      0.983   0.99859    1.0364\nsex                              0.58318      1.715   0.41659    0.8164\nns(wt.loss, knots = c(1, 12))1   0.80492      1.242   0.30240    2.1425\nns(wt.loss, knots = c(1, 12))2   0.05127     19.503   0.00192    1.3696\nns(wt.loss, knots = c(1, 12))3   0.28691      3.485   0.04478    1.8381\n\nConcordance= 0.61  (se = 0.025 )\nLikelihood ratio test= 18.76  on 5 df,   p=0.002\nWald test            = 17.91  on 5 df,   p=0.003\nScore (logrank) test = 18.42  on 5 df,   p=0.002\n\n\nWe see that weight loss has three coefficients, one for the lowest value through 1 (1), one for 1 through 12 (2), and one for 12 through the highest value (3). Also note that we do not have an intercept: to calculate the final predicted risk, we need to use the baseline hazard instead of the intercept. However, the baseline hazard is different for each timepoint.\nWe can use basehaz() to calculate the baseline hazard at our timepoint of interest (2 years):\n\n# Retrieve baseline hazard at 2 years\nfilter(basehaz(fit), time == 365.25 * 2)[[\"hazard\"]]\n\n[1] 2.146102\n\n\nDiscrimination of this model can be calculated using Harrell’s C-statistic: * The function cindex() from the {dynpred} package (does not require model fit, but does require development formula). * The function rcorr.cens() from the {Hmisc} package (does not require model fit, but may be slow). * The function cIndex() from the {intsurv} package.\nThe calibration slope can be calculated as before, using coxph() and a calibration can also be drawn using the same methods as discussed before. In the case of competing risks, take note of validation measures for competing risk settings. More on that can be found in Van Geloven et al. 2021 and Ramspek et al. 2021.",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#method-ii-cph",
    "href": "pint_prd1.html#method-ii-cph",
    "title": "PINT - Prediction Practical 1",
    "section": "6.3 Method II: cph()",
    "text": "6.3 Method II: cph()\nWe can also use cph() from {rms}. Although this is an extension of coxph() from {survival}, the returned model fit object can be used with the validate() and calibrate() functions from {rms}:\n\n# Develop the prediction model\nfit &lt;- cph(Surv(time, status) ~                                 \n               age +                                            \n               sex +\n1               rcs(wt.loss),\n           data = lung)                       \n\n\n1\n\nThis time, we use rcs() from {rms} instead of ns() from {splines}. The advantage of rcs() is that it does not transform the underlying variable (i.e. not a B-spline matrix basis for a natural cubic spline) and is therefore easier to interpret and write down in a concise formula. However, we are forced to use at least 3 knots meaning more coefficients are estimated, whilst ns() allows us to use a single knot. Note that we can both use rcs() and ns() in both coxph() and cph().\n\n\n\n\nnumber of knots in rcs defaulting to 5",
    "crumbs": [
      "PINT practical 1"
    ]
  },
  {
    "objectID": "pint_prd1.html#the-curse-of-the-model-fit-object-revisited",
    "href": "pint_prd1.html#the-curse-of-the-model-fit-object-revisited",
    "title": "PINT - Prediction Practical 1",
    "section": "6.4 The curse of the model fit object revisited",
    "text": "6.4 The curse of the model fit object revisited\nWe already saw that with the model fit object, validation of a prediction model becomes quite easy.\n\n\n\n\n\n\nUsing predict()\n\n\n\nIf we use predict() to calculate the survival probability for an individual based on a Cox regression model (thus using predict(fit, type = \"survival\"), note that the survival probability is not the probability at our specified time horizon, but at the end of their observed follow-up. Therefore, we cannot use this to calculate predictions (see also this issue opened by the author of this practical, who turned out to be wrong but learned along the way)\nTo get individual predictions, instead use predict(fit, type = \"lp\") and calculate the survival probability from the linear predictor using \\(\\hat{y}(t) = 1 - S_0(t)^{e^{LP_i}}\\) where \\(S_0(t) = e^{-H_0(t)}\\) and \\(H_0(t)\\) is the baseline hazard.\n\n\nHowever, in the absence of a model fit object, the validator may struggle more, especially for Cox regression models. By default, both coxph() and cph() estimate a centered Cox regression model. This means that the reference individual for the model is not someone without any binary predictor and with all continuous variables at 0, but someone without any binary predictor and with all continuous variables at the mean of that variable. If we then want to calculate an individual’s linear predictor, we also should have the linear predictor of the sample (\\(LP_{sample}\\)). To calculate an individual’s linear predictor for the final prediction, we substract \\(LP_{sample}\\) from their individual linear predictor \\(LP_{individual}\\): \\(\\hat{y}(t) = 1 - S_0(t)^{e^{LP_{individual} - LP_{sample}}}\\). More on this can be found in this blogpost.",
    "crumbs": [
      "PINT practical 1"
    ]
  }
]